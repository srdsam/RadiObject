{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ML Training with MSD Lung RadiObject\n",
    "\n",
    "This notebook trains a binary classifier for lung tumor detection using the Medical Segmentation Decathlon data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Load** RadiObject from URI (S3 or local)\n",
    "2. **Explore** data and label distribution\n",
    "3. **Split** into train/validation sets\n",
    "4. **Train** a 3D CNN classifier\n",
    "5. **Evaluate** model performance\n",
    "\n",
    "## Task\n",
    "\n",
    "Binary classification: Predict `has_tumor` (0 or 1) from CT volume patches.\n",
    "\n",
    "**Prerequisites:** Run [05_ingest_msd.ipynb](./05_ingest_msd.ipynb) first to create the MSD Lung RadiObject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": "import tempfile\nimport shutil\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom config import MSD_LUNG_URI, S3_REGION\nfrom radiobject import RadiObject, configure\nfrom radiobject.ctx import S3Config\nfrom radiobject.ml import (\n    create_training_dataloader,\n    create_validation_dataloader,\n    Compose,\n    IntensityNormalize,\n    RandomFlip3D,\n)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"RadiObject URI: {MSD_LUNG_URI}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine compute device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Training device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 access if using S3 URI\n",
    "if MSD_LUNG_URI.startswith(\"s3://\"):\n",
    "    configure(s3=S3Config(region=S3_REGION, max_parallel_ops=8))\n",
    "\n",
    "TEMP_DIR = tempfile.mkdtemp(prefix=\"msd_ml_\")\n",
    "print(f\"Temp directory: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 2. Load RadiObject from URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-radiobject",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RadiObject\n",
    "radi = RadiObject(MSD_LUNG_URI)\n",
    "\n",
    "print(f\"RadiObject: {radi}\")\n",
    "print(f\"Subjects: {len(radi)}\")\n",
    "print(f\"Collections: {radi.collection_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-obs-meta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display subject metadata\n",
    "obs_meta = radi.obs_meta.read()\n",
    "print(f\"obs_meta columns: {list(obs_meta.columns)}\")\n",
    "obs_meta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "## 3. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "label_counts = obs_meta['has_tumor'].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "label_counts.plot(kind='bar', ax=ax, color=['steelblue', 'coral'])\n",
    "ax.set_xlabel('has_tumor')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Label Distribution')\n",
    "ax.set_xticklabels(['No Tumor (0)', 'Has Tumor (1)'], rotation=0)\n",
    "\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    ax.text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label distribution: {label_counts.to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples with and without tumor\n",
    "collection_name = radi.collection_names[0]\n",
    "vc = radi.collection(collection_name)\n",
    "\n",
    "# Get indices for each class\n",
    "tumor_subjects = obs_meta[obs_meta['has_tumor'] == 1]['obs_subject_id'].tolist()\n",
    "no_tumor_subjects = obs_meta[obs_meta['has_tumor'] == 0]['obs_subject_id'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# No tumor samples\n",
    "for i, subject_id in enumerate(no_tumor_subjects[:3]):\n",
    "    vol = radi.loc[subject_id].collection(collection_name).iloc[0]\n",
    "    mid_z = vol.shape[2] // 2\n",
    "    axes[0, i].imshow(vol.axial(z=mid_z).T, cmap='gray', origin='lower')\n",
    "    axes[0, i].set_title(f'{subject_id} (no tumor)')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Tumor samples\n",
    "for i, subject_id in enumerate(tumor_subjects[:3]):\n",
    "    vol = radi.loc[subject_id].collection(collection_name).iloc[0]\n",
    "    mid_z = vol.shape[2] // 2\n",
    "    axes[1, i].imshow(vol.axial(z=mid_z).T, cmap='gray', origin='lower')\n",
    "    axes[1, i].set_title(f'{subject_id} (has tumor)')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample CT Scans by Label', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "## 4. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-val-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 stratified split\n",
    "all_ids = list(radi.obs_subject_ids)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(all_ids)\n",
    "\n",
    "split_idx = int(0.8 * len(all_ids))\n",
    "train_ids = all_ids[:split_idx]\n",
    "val_ids = all_ids[split_idx:]\n",
    "\n",
    "print(f\"Training subjects: {len(train_ids)}\")\n",
    "print(f\"Validation subjects: {len(val_ids)}\")\n",
    "\n",
    "# Check label distribution in splits\n",
    "train_labels = obs_meta[obs_meta['obs_subject_id'].isin(train_ids)]['has_tumor']\n",
    "val_labels = obs_meta[obs_meta['obs_subject_id'].isin(val_ids)]['has_tumor']\n",
    "\n",
    "print(f\"\\nTrain label distribution: {train_labels.value_counts().to_dict()}\")\n",
    "print(f\"Val label distribution: {val_labels.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val RadiObjects\n",
    "train_uri = f\"{TEMP_DIR}/train_radi\"\n",
    "val_uri = f\"{TEMP_DIR}/val_radi\"\n",
    "\n",
    "radi_train = radi.loc[train_ids].to_radi_object(train_uri)\n",
    "radi_val = radi.loc[val_ids].to_radi_object(val_uri)\n",
    "\n",
    "print(f\"Train RadiObject: {radi_train}\")\n",
    "print(f\"Val RadiObject: {radi_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataloader-header",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataloaders",
   "metadata": {},
   "outputs": [],
   "source": "# Training hyperparameters\nBATCH_SIZE = 4\nPATCH_SIZE = (64, 64, 64)\n\n# Define transforms\ntrain_transform = Compose([\n    IntensityNormalize(),\n    RandomFlip3D(axes=(0, 1, 2), prob=0.5),\n])\n\nval_transform = Compose([\n    IntensityNormalize(),\n])\n\n# Create dataloaders using the new API\ntrain_loader = create_training_dataloader(\n    radi_train,\n    modalities=[radi.collection_names[0]],\n    label_column=\"has_tumor\",\n    batch_size=BATCH_SIZE,\n    patch_size=PATCH_SIZE,\n    num_workers=0,\n    pin_memory=False,\n    persistent_workers=False,\n    transform=train_transform,\n)\n\n# Use create_validation_dataloader for val set (no shuffle, no drop_last)\nval_loader = create_validation_dataloader(\n    radi_val,\n    modalities=[radi.collection_names[0]],\n    label_column=\"has_tumor\",\n    batch_size=BATCH_SIZE,\n    patch_size=PATCH_SIZE,\n    num_workers=0,\n    pin_memory=False,\n    transform=val_transform,\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\nprint(f\"Train transform: {train_transform}\")\nprint(f\"Val transform: {val_transform}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch keys: {list(batch.keys())}\")\n",
    "print(f\"Image shape: {batch['image'].shape}\")  # (B, C, D, H, W)\n",
    "print(f\"Image dtype: {batch['image'].dtype}\")\n",
    "print(f\"Labels: {batch['label'].tolist()}\")\n",
    "print(f\"Memory per batch: {batch['image'].nbytes / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 6. Define Model\n",
    "\n",
    "A simple 3D CNN classifier with three convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple3DCNN(nn.Module):\n",
    "    \"\"\"3D CNN for binary classification.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(64)\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = Simple3DCNN(in_channels=1, num_classes=2).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training on {DEVICE} for {NUM_EPOCHS} epochs...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images = batch[\"image\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].long().to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"image\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].long().to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    # Record metrics\n",
    "    history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "    history[\"train_acc\"].append(100.0 * train_correct / train_total)\n",
    "    history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "    history[\"val_acc\"].append(100.0 * val_correct / val_total)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1:2d}/{NUM_EPOCHS}: \"\n",
    "        f\"Train Loss={history['train_loss'][-1]:.4f}, \"\n",
    "        f\"Train Acc={history['train_acc'][-1]:.1f}%, \"\n",
    "        f\"Val Loss={history['val_loss'][-1]:.4f}, \"\n",
    "        f\"Val Acc={history['val_acc'][-1]:.1f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "axes[0].plot(history[\"val_loss\"], marker=\"s\", label=\"Validation\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training & Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history[\"train_acc\"], marker=\"o\", label=\"Train\")\n",
    "axes[1].plot(history[\"val_acc\"], marker=\"s\", label=\"Validation\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[1].set_title(\"Training & Validation Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final metrics\n",
    "print(\"=\" * 40)\n",
    "print(\"Final Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Best Train Accuracy: {max(history['train_acc']):.1f}%\")\n",
    "print(f\"Best Val Accuracy: {max(history['val_acc']):.1f}%\")\n",
    "print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary directory\n",
    "shutil.rmtree(TEMP_DIR)\n",
    "print(f\"Cleaned up: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated:\n\n1. **Loading** RadiObject from configured URI (S3 or local)\n2. **Exploring** data and label distributions\n3. **Splitting** data into train/validation sets\n4. **Training** a 3D CNN classifier with PyTorch\n5. **Evaluating** model performance\n\n### ML Framework Features\n\n| Feature | Example |\n|---------|---------|\n| **Training dataloader** | `create_training_dataloader(radi, ...)` - shuffled, drop_last |\n| **Validation dataloader** | `create_validation_dataloader(radi, ...)` - no shuffle |\n| **Transform composition** | `Compose([IntensityNormalize(), RandomFlip3D()])` |\n| **Patch extraction** | Efficient 64³ patches from ISOTROPIC-tiled volumes |\n| **Label integration** | Automatic obs_meta lookup via `label_column` |\n\n### RadiObject Benefits for ML Training\n\n| Feature | Benefit |\n|---------|---------|\n| **ISOTROPIC tiles** | 64³ chunks optimized for random patch access |\n| **TileDB storage** | Memory-mapped I/O, only loads requested data |\n| **S3 support** | Direct cloud storage access |\n| **Compression** | 3-10x storage reduction with ZSTD |\n\n### Next Steps\n\n- Increase `NUM_EPOCHS` for better convergence\n- Try more augmentations: `RandomNoise`, `WindowLevel` for CT\n- Use `CacheStrategy.IN_MEMORY` for small datasets\n- Experiment with different architectures (ResNet3D, UNet, etc.)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}