{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Medical Segmentation Decathlon: Lung Tumor Data Ingestion\n",
    "\n",
    "This notebook downloads the Medical Segmentation Decathlon (MSD) Task06_Lung dataset and ingests it into RadiObject for ML training.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Task** | Task06_Lung |\n",
    "| **Modality** | CT |\n",
    "| **Subjects** | 63 training + 32 test |\n",
    "| **Labels** | Lung tumor segmentation masks |\n",
    "| **Size** | ~8.5GB (compressed tar) |\n",
    "| **License** | CC-BY-SA 4.0 |\n",
    "\n",
    "We derive a binary classification label (`has_tumor`) from the segmentation masks.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Edit `config.py` to change the target URI:\n",
    "\n",
    "```python\n",
    "# S3 storage (default)\n",
    "MSD_LUNG_URI = \"s3://souzy-scratch/msd-lung/radiobject\"\n",
    "\n",
    "# Local storage alternative\n",
    "MSD_LUNG_URI = \"./data/msd-lung\"\n",
    "```\n",
    "\n",
    "**Reference**: [Medical Segmentation Decathlon](http://medicaldecathlon.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import MSD_LUNG_URI, S3_REGION\n",
    "from radiobject.radi_object import RadiObject\n",
    "from radiobject.ctx import configure, S3Config\n",
    "\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"NiBabel: {nib.__version__}\")\n",
    "print(f\"Target URI: {MSD_LUNG_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# Configure S3 access if using S3 URI\nif MSD_LUNG_URI.startswith(\"s3://\"):\n    configure(s3=S3Config(region=S3_REGION, max_parallel_ops=8))\n\n# Configure ISOTROPIC tile layout for ML training (64³ chunks for efficient patch extraction)\nfrom radiobject.ctx import TileConfig, SliceOrientation\nconfigure(tile=TileConfig(orientation=SliceOrientation.ISOTROPIC))\n\n# S3 URIs for backup (optional)\nS3_BUCKET = \"s3://souzy-scratch/msd-lung\"\nS3_NIFTI_URI = f\"{S3_BUCKET}/nifti\"  # Backup of raw NIfTIs\n\n# Local paths (temporary, for processing)\nDATA_DIR = Path(\"../data/msd_lung\")\nTAR_PATH = DATA_DIR / \"Task06_Lung.tar\"\nTASK_DIR = DATA_DIR / \"Task06_Lung\"\n\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nprint(f\"Local data directory: {DATA_DIR.resolve()}\")\nprint(f\"Target RadiObject URI: {MSD_LUNG_URI}\")\nprint(f\"Tile orientation: ISOTROPIC (64³ chunks for ML patch extraction)\")"
  },
  {
   "cell_type": "markdown",
   "id": "check-header",
   "metadata": {},
   "source": [
    "## 2. Check if RadiObject Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-exists",
   "metadata": {},
   "outputs": [],
   "source": "def uri_exists(uri: str) -> bool:\n    \"\"\"Check if RadiObject exists at URI.\"\"\"\n    try:\n        radi = RadiObject(uri)\n        _ = radi.collection_names  # Force validation by accessing group metadata\n        return True\n    except Exception:\n        return False\n\nif uri_exists(MSD_LUNG_URI):\n    print(f\"RadiObject already exists at {MSD_LUNG_URI}\")\n    print(\"Skipping ingestion. Delete the URI to re-ingest.\")\n    SKIP_INGESTION = True\nelse:\n    print(f\"No RadiObject found at {MSD_LUNG_URI}\")\n    print(\"Proceeding with ingestion...\")\n    SKIP_INGESTION = False"
  },
  {
   "cell_type": "markdown",
   "id": "download-header",
   "metadata": {},
   "source": [
    "## 3. Download MSD Task06_Lung\n",
    "\n",
    "The dataset is hosted on AWS S3 with anonymous access (no authentication required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": "if not SKIP_INGESTION:\n    # Download from S3 backup (faster and more reliable than original MSD source)\n    if not TASK_DIR.exists():\n        print(f\"Downloading NIfTIs from S3 backup: {S3_NIFTI_URI}...\")\n        TASK_DIR.mkdir(parents=True, exist_ok=True)\n        result = subprocess.run(\n            [\"aws\", \"s3\", \"sync\", S3_NIFTI_URI, str(TASK_DIR)],\n            capture_output=True,\n            text=True,\n        )\n        if result.returncode != 0:\n            raise RuntimeError(f\"Download failed: {result.stderr}\")\n        print(\"Download complete.\")\n    else:\n        print(f\"Task directory already exists at {TASK_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract",
   "metadata": {},
   "outputs": [],
   "source": "if not SKIP_INGESTION:\n    # Verify download (no extraction needed - we downloaded NIfTIs directly)\n    dataset_json = TASK_DIR / \"dataset.json\"\n    if not dataset_json.exists():\n        raise FileNotFoundError(f\"dataset.json not found at {dataset_json}\")\n    print(f\"Found dataset.json at {dataset_json}\")"
  },
  {
   "cell_type": "markdown",
   "id": "parse-header",
   "metadata": {},
   "source": [
    "## 4. Parse Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Load dataset.json\n",
    "    with open(TASK_DIR / \"dataset.json\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    print(f\"Dataset name: {metadata.get('name', 'N/A')}\")\n",
    "    print(f\"Description: {metadata.get('description', 'N/A')}\")\n",
    "    print(f\"Modality: {metadata.get('modality', 'N/A')}\")\n",
    "    print(f\"Labels: {metadata.get('labels', 'N/A')}\")\n",
    "    print(f\"Tensor image size: {metadata.get('tensorImageSize', 'N/A')}\")\n",
    "    print(f\"Training samples: {metadata.get('numTraining', len(metadata.get('training', [])))}\")\n",
    "    print(f\"Test samples: {metadata.get('numTest', len(metadata.get('test', [])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Parse training samples\n",
    "    training = metadata[\"training\"]\n",
    "    print(f\"\\nFirst 3 training entries:\")\n",
    "    for entry in training[:3]:\n",
    "        print(f\"  Image: {entry['image']}, Label: {entry['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labels-header",
   "metadata": {},
   "source": [
    "## 5. Create obs_meta with Binary Labels\n",
    "\n",
    "We derive `has_tumor` by checking if any voxel in the segmentation mask is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-obs-meta",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    obs_data = []\n",
    "\n",
    "    for i, entry in enumerate(training):\n",
    "        # Handle path format (may have \"./\" prefix)\n",
    "        image_rel = entry[\"image\"].lstrip(\"./\")\n",
    "        label_rel = entry[\"label\"].lstrip(\"./\")\n",
    "        \n",
    "        image_path = TASK_DIR / image_rel\n",
    "        label_path = TASK_DIR / label_rel\n",
    "        \n",
    "        # Derive subject_id from filename (e.g., \"lung_001\" from \"imagesTr/lung_001.nii.gz\")\n",
    "        filename = Path(image_rel).name\n",
    "        subject_id = filename.replace(\".nii.gz\", \"\").replace(\".nii\", \"\")\n",
    "        \n",
    "        # Load label mask to determine has_tumor\n",
    "        label_img = nib.load(label_path)\n",
    "        label_data = np.asarray(label_img.dataobj)\n",
    "        has_tumor = int(np.any(label_data > 0))\n",
    "        \n",
    "        # Get image metadata\n",
    "        img = nib.load(image_path)\n",
    "        shape = img.shape\n",
    "        zooms = img.header.get_zooms()\n",
    "        \n",
    "        obs_data.append({\n",
    "            \"obs_subject_id\": subject_id,\n",
    "            \"obs_id\": subject_id,\n",
    "            \"has_tumor\": has_tumor,\n",
    "            \"shape_x\": shape[0],\n",
    "            \"shape_y\": shape[1],\n",
    "            \"shape_z\": shape[2],\n",
    "            \"spacing_x\": float(zooms[0]),\n",
    "            \"spacing_y\": float(zooms[1]),\n",
    "            \"spacing_z\": float(zooms[2]),\n",
    "            \"image_path\": str(image_path),\n",
    "            \"label_path\": str(label_path),\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(training)} samples...\")\n",
    "\n",
    "    obs_df = pd.DataFrame(obs_data)\n",
    "    print(f\"\\nTotal samples: {len(obs_df)}\")\n",
    "    print(f\"Label distribution: {obs_df['has_tumor'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-obs-meta",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Display sample metadata\n",
    "    display(obs_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "## 6. Explore Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Load and visualize a sample with tumor\n",
    "    tumor_samples = obs_df[obs_df['has_tumor'] == 1]\n",
    "    sample = tumor_samples.iloc[0] if len(tumor_samples) > 0 else obs_df.iloc[0]\n",
    "\n",
    "    img = nib.load(sample[\"image_path\"])\n",
    "    img_data = np.asarray(img.dataobj)\n",
    "\n",
    "    label = nib.load(sample[\"label_path\"])\n",
    "    label_data = np.asarray(label.dataobj)\n",
    "\n",
    "    print(f\"Subject: {sample['obs_subject_id']}\")\n",
    "    print(f\"Has tumor: {sample['has_tumor']}\")\n",
    "    print(f\"Image shape: {img_data.shape}\")\n",
    "    print(f\"Image dtype: {img_data.dtype}\")\n",
    "    print(f\"Value range: [{img_data.min():.0f}, {img_data.max():.0f}]\")\n",
    "    print(f\"Label unique values: {np.unique(label_data)}\")\n",
    "    print(f\"Voxel spacing: {img.header.get_zooms()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Find slice with tumor\n",
    "    if sample['has_tumor'] == 1:\n",
    "        # Find z-slice with maximum tumor area\n",
    "        tumor_area_per_slice = (label_data > 0).sum(axis=(0, 1))\n",
    "        best_z = int(np.argmax(tumor_area_per_slice))\n",
    "    else:\n",
    "        best_z = img_data.shape[2] // 2\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # CT image\n",
    "    axes[0].imshow(img_data[:, :, best_z].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[0].set_title(f\"CT - Axial (z={best_z})\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Segmentation mask\n",
    "    axes[1].imshow(label_data[:, :, best_z].T, cmap=\"hot\", origin=\"lower\")\n",
    "    axes[1].set_title(\"Tumor Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Overlay\n",
    "    axes[2].imshow(img_data[:, :, best_z].T, cmap=\"gray\", origin=\"lower\")\n",
    "    mask_overlay = np.ma.masked_where(label_data[:, :, best_z].T == 0, label_data[:, :, best_z].T)\n",
    "    axes[2].imshow(mask_overlay, cmap=\"Reds\", alpha=0.5, origin=\"lower\")\n",
    "    axes[2].set_title(\"Overlay\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Subject: {sample['obs_subject_id']} (has_tumor={sample['has_tumor']})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backup-header",
   "metadata": {},
   "source": [
    "## 7. Backup Raw NIfTIs to S3 (Optional)\n",
    "\n",
    "Upload the raw NIfTI files to S3 so you can access them from any machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backup-nifti",
   "metadata": {},
   "outputs": [],
   "source": "if not SKIP_INGESTION:\n    # Skip S3 backup - we already downloaded from S3 backup\n    print(\"Data loaded from S3 backup - no additional backup needed.\")"
  },
  {
   "cell_type": "markdown",
   "id": "ingest-header",
   "metadata": {},
   "source": [
    "## 8. Create RadiObject from NIfTIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-radiobject",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Prepare NIfTI list for ingestion\n",
    "    nifti_list = [\n",
    "        (row[\"image_path\"], row[\"obs_subject_id\"])\n",
    "        for _, row in obs_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Subject-level metadata (exclude file paths, keep shape and spacing info)\n",
    "    obs_meta_df = obs_df[[\n",
    "        \"obs_subject_id\", \"obs_id\", \"has_tumor\",\n",
    "        \"shape_x\", \"shape_y\", \"shape_z\",\n",
    "        \"spacing_x\", \"spacing_y\", \"spacing_z\",\n",
    "    ]].copy()\n",
    "\n",
    "    print(f\"Creating RadiObject with {len(nifti_list)} volumes...\")\n",
    "    print(f\"Target URI: {MSD_LUNG_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Create RadiObject (stored on S3 or local)\n",
    "    radi = RadiObject.from_niftis(\n",
    "        uri=MSD_LUNG_URI,\n",
    "        niftis=nifti_list,\n",
    "        obs_meta=obs_meta_df,\n",
    "        reorient=True,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nRadiObject created: {radi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate-header",
   "metadata": {},
   "source": [
    "## 9. Validate & Explore RadiObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Validate integrity\n",
    "    radi.validate()\n",
    "    print(\"Validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-radi",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Explore the RadiObject\n",
    "    print(f\"Number of subjects: {len(radi)}\")\n",
    "    print(f\"Collections: {radi.collection_names}\")\n",
    "    print(f\"\\nobs_meta:\")\n",
    "    display(radi.obs_meta.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Verify data by reading a sample\n",
    "    collection_name = radi.collection_names[0]\n",
    "    vc = radi.collection(collection_name)\n",
    "\n",
    "    print(f\"Collection: {collection_name}\")\n",
    "    print(f\"Shape: {vc.shape}\")\n",
    "    print(f\"Number of volumes: {len(vc)}\")\n",
    "\n",
    "    # Read a slice\n",
    "    sample_vol = vc.iloc[0]\n",
    "    mid_z = sample_vol.shape[2] // 2\n",
    "    axial_slice = sample_vol.axial(z=mid_z)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(axial_slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "    plt.title(f\"RadiObject sample: {collection_name} (z={mid_z})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## 10. Cleanup Local Data\n",
    "\n",
    "Remove local NIfTI files after successful upload to S3. Data is now accessible from any machine via the S3 URIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_INGESTION:\n",
    "    # Remove local data (data is now in S3)\n",
    "    CLEANUP_LOCAL = False  # Set to True to remove local files\n",
    "\n",
    "    if CLEANUP_LOCAL:\n",
    "        shutil.rmtree(DATA_DIR)\n",
    "        print(f\"Removed local data: {DATA_DIR}\")\n",
    "    else:\n",
    "        print(f\"Local data retained at: {DATA_DIR.resolve()}\")\n",
    "        print(\"Set CLEANUP_LOCAL = True to remove after confirming S3 upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-final-header",
   "metadata": {},
   "source": [
    "## 11. Verify RadiObject\n",
    "\n",
    "Load the RadiObject from the URI to verify it was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from URI (works whether we just created it or it already existed)\n",
    "radi = RadiObject(MSD_LUNG_URI)\n",
    "\n",
    "print(f\"Loaded: {radi}\")\n",
    "print(f\"Collections: {radi.collection_names}\")\n",
    "print(f\"Subjects: {len(radi)}\")\n",
    "\n",
    "# Quick data check\n",
    "collection_name = radi.collection_names[0]\n",
    "vol = radi.collection(collection_name).iloc[0]\n",
    "print(f\"\\nSample volume: {vol}\")\n",
    "print(f\"Axial slice shape: {vol.axial(z=vol.shape[2]//2).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. **Checked** if RadiObject already exists (skip if so)\n",
    "2. **Downloaded** MSD Task06_Lung from public AWS S3\n",
    "3. **Extracted** tar archive and parsed dataset.json\n",
    "4. **Derived** binary `has_tumor` labels from segmentation masks\n",
    "5. **Captured** full metadata: shape, voxel spacing, labels\n",
    "6. **Backed up** raw NIfTIs to your S3 bucket (optional)\n",
    "7. **Created** RadiObject with NIfTI volumes and metadata\n",
    "\n",
    "### S3 Locations (default)\n",
    "\n",
    "| Resource | URI |\n",
    "|----------|-----|\n",
    "| RadiObject | `s3://souzy-scratch/msd-lung/radiobject` |\n",
    "| Raw NIfTIs | `s3://souzy-scratch/msd-lung/nifti/` |\n",
    "\n",
    "### Dataset Statistics\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Total subjects | 63 (training set) |\n",
    "| Modality | CT |\n",
    "| Labels | `has_tumor` (binary) |\n",
    "| Metadata | shape, voxel spacing |\n",
    "| Storage | S3 with TileDB compression |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [06_ml_training.ipynb](./06_ml_training.ipynb) - Train a tumor classifier using this RadiObject\n",
    "\n",
    "### Loading from URI on Another Machine\n",
    "\n",
    "```python\n",
    "from config import MSD_LUNG_URI, S3_REGION\n",
    "from radiobject.radi_object import RadiObject\n",
    "from radiobject.ctx import configure, S3Config\n",
    "\n",
    "if MSD_LUNG_URI.startswith(\"s3://\"):\n",
    "    configure(s3=S3Config(region=S3_REGION))\n",
    "\n",
    "radi = RadiObject(MSD_LUNG_URI)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}