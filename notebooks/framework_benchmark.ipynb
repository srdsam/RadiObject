{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Framework Benchmark: RadiObject vs MONAI vs TorchIO\n",
    "\n",
    "Publication-quality benchmark comparing medical imaging data loading frameworks using the LIDC-IDRI dataset.\n",
    "\n",
    "## Benchmark Scenarios\n",
    "\n",
    "### Scenario A: Local Storage (Fair Comparison)\n",
    "All frameworks read from local filesystem:\n",
    "- **RadiObject**: Local TileDB array\n",
    "- **MONAI**: Local NIfTI files\n",
    "- **TorchIO**: Local NIfTI files\n",
    "\n",
    "### Scenario B: S3 Remote Storage (RadiObject Advantage)\n",
    "Demonstrates RadiObject's unique S3 capability via TileDB:\n",
    "- **RadiObject (S3)**: S3-backed TileDB array\n",
    "- **RadiObject (Local)**: Baseline comparison\n",
    "\n",
    "## Metrics\n",
    "| Metric | Description | Unit |\n",
    "|--------|-------------|------|\n",
    "| Throughput | Samples loaded per second | samples/sec |\n",
    "| Cold Start | First batch load time | seconds |\n",
    "| Warm Load | Subsequent batch load time | seconds |\n",
    "| Peak Memory | Memory during loading | MB |\n",
    "| Epoch Time | Full iteration over dataset | seconds |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-specs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "import psutil\n",
    "import time\n",
    "import gc\n",
    "import tracemalloc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MACHINE SPECIFICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "\n",
    "try:\n",
    "    chip = subprocess.check_output([\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"], text=True).strip()\n",
    "    print(f\"CPU: {chip}\")\n",
    "except Exception:\n",
    "    print(f\"CPU: {platform.processor()}\")\n",
    "\n",
    "print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-frameworks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "HAVE_MONAI = False\n",
    "HAVE_TORCHIO = False\n",
    "HAVE_SIMPLEITK = False\n",
    "\n",
    "try:\n",
    "    import monai\n",
    "    from monai.data import Dataset as MonaiDataset, DataLoader as MonaiDataLoader\n",
    "    from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, RandSpatialCropd\n",
    "    HAVE_MONAI = True\n",
    "    print(f\"MONAI: {monai.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"MONAI: Not installed\")\n",
    "\n",
    "try:\n",
    "    import torchio as tio\n",
    "    HAVE_TORCHIO = True\n",
    "    print(f\"TorchIO: {tio.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"TorchIO: Not installed\")\n",
    "\n",
    "try:\n",
    "    import SimpleITK as sitk\n",
    "    HAVE_SIMPLEITK = True\n",
    "    print(f\"SimpleITK: {sitk.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"SimpleITK: Not installed\")\n",
    "\n",
    "from radiobject.radi_object import RadiObject\n",
    "from radiobject.ctx import configure, S3Config, get_config\n",
    "from ml import create_training_dataloader\n",
    "\n",
    "print(\"\\nRadiObject: Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env-config",
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark Configuration\nBATCH_SIZE = 4\nPATCH_SIZE = (64, 64, 64)\nNUM_WORKERS = 0  # Single-threaded for fair comparison\nN_WARMUP = 3\nN_BATCHES = 20\nN_RUNS = 5\n\n# Data paths\nDATA_DIR = Path(\"../data\")\nDICOM_DIR = DATA_DIR / \"lidc_subset\"\nNIFTI_DIR = DATA_DIR / \"nifti\"\nRADIOBJECT_LOCAL_URI = str(DATA_DIR / \"radiobject_local\")\nASSETS_DIR = Path(\"../assets/benchmark\")\n\n# S3 config - use existing RadiObject in S3\nS3_BUCKET = os.environ.get(\"RADIOBJECT_S3_BUCKET\", \"souzy-scratch\")\nRADIOBJECT_S3_URI = f\"s3://{S3_BUCKET}/lidc-idri/radiobject\" if S3_BUCKET else \"\"\n\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nDICOM_DIR.mkdir(parents=True, exist_ok=True)\nNIFTI_DIR.mkdir(parents=True, exist_ok=True)\nASSETS_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Patch size: {PATCH_SIZE}\")\nprint(f\"Warmup iterations: {N_WARMUP}\")\nprint(f\"Benchmark batches: {N_BATCHES}\")\nprint(f\"Runs per framework: {N_RUNS}\")\nprint(f\"S3 RadiObject URI: {RADIOBJECT_S3_URI}\")"
  },
  {
   "cell_type": "markdown",
   "id": "utils-header",
   "metadata": {},
   "source": [
    "## 2. Benchmark Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils-classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Single benchmark run result.\"\"\"\n",
    "    framework: str\n",
    "    scenario: str\n",
    "    cold_start_s: float\n",
    "    batch_times_s: list[float]\n",
    "    peak_memory_mb: float\n",
    "    \n",
    "    @property\n",
    "    def throughput(self) -> float:\n",
    "        return BATCH_SIZE / np.mean(self.batch_times_s)\n",
    "    \n",
    "    @property\n",
    "    def mean_batch_s(self) -> float:\n",
    "        return np.mean(self.batch_times_s)\n",
    "    \n",
    "    @property\n",
    "    def std_batch_s(self) -> float:\n",
    "        return np.std(self.batch_times_s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AggregatedResult:\n",
    "    \"\"\"Aggregated results from multiple runs.\"\"\"\n",
    "    framework: str\n",
    "    scenario: str\n",
    "    throughput_mean: float\n",
    "    throughput_std: float\n",
    "    cold_start_mean: float\n",
    "    cold_start_std: float\n",
    "    batch_time_mean: float\n",
    "    batch_time_std: float\n",
    "    peak_memory_mean: float\n",
    "    peak_memory_std: float\n",
    "\n",
    "\n",
    "def aggregate_results(results: list[BenchmarkResult]) -> AggregatedResult:\n",
    "    \"\"\"Aggregate multiple benchmark runs.\"\"\"\n",
    "    throughputs = [r.throughput for r in results]\n",
    "    cold_starts = [r.cold_start_s for r in results]\n",
    "    batch_times = [r.mean_batch_s for r in results]\n",
    "    memories = [r.peak_memory_mb for r in results]\n",
    "    \n",
    "    return AggregatedResult(\n",
    "        framework=results[0].framework,\n",
    "        scenario=results[0].scenario,\n",
    "        throughput_mean=np.mean(throughputs),\n",
    "        throughput_std=np.std(throughputs),\n",
    "        cold_start_mean=np.mean(cold_starts),\n",
    "        cold_start_std=np.std(cold_starts),\n",
    "        batch_time_mean=np.mean(batch_times),\n",
    "        batch_time_std=np.std(batch_times),\n",
    "        peak_memory_mean=np.mean(memories),\n",
    "        peak_memory_std=np.std(memories),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataloader_benchmark(\n",
    "    loader: DataLoader,\n",
    "    framework: str,\n",
    "    scenario: str,\n",
    "    image_key: str = \"image\",\n",
    ") -> BenchmarkResult:\n",
    "    \"\"\"Run a single benchmark iteration on a DataLoader.\"\"\"\n",
    "    gc.collect()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    # Cold start\n",
    "    loader_iter = iter(loader)\n",
    "    cold_start = time.perf_counter()\n",
    "    first_batch = next(loader_iter)\n",
    "    if isinstance(first_batch, dict):\n",
    "        _ = first_batch[image_key].shape\n",
    "    else:\n",
    "        _ = first_batch.shape\n",
    "    cold_start_time = time.perf_counter() - cold_start\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(N_WARMUP - 1):\n",
    "        try:\n",
    "            batch = next(loader_iter)\n",
    "        except StopIteration:\n",
    "            loader_iter = iter(loader)\n",
    "            batch = next(loader_iter)\n",
    "    \n",
    "    # Benchmark\n",
    "    batch_times = []\n",
    "    for i in range(N_BATCHES):\n",
    "        try:\n",
    "            batch_start = time.perf_counter()\n",
    "            batch = next(loader_iter)\n",
    "            if isinstance(batch, dict):\n",
    "                _ = batch[image_key].shape\n",
    "            else:\n",
    "                _ = batch.shape\n",
    "            batch_times.append(time.perf_counter() - batch_start)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    return BenchmarkResult(\n",
    "        framework=framework,\n",
    "        scenario=scenario,\n",
    "        cold_start_s=cold_start_time,\n",
    "        batch_times_s=batch_times,\n",
    "        peak_memory_mb=peak / (1024 * 1024),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Download a subset of LIDC-IDRI from S3 and prepare data for all frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing DICOM data\n",
    "existing_dcm = list(DICOM_DIR.rglob(\"*.dcm\"))\n",
    "print(f\"Existing DICOM files: {len(existing_dcm)}\")\n",
    "\n",
    "# Check if we need to download from S3\n",
    "if len(existing_dcm) < 100:\n",
    "    print(\"\\nDownloading LIDC-IDRI subset from S3...\")\n",
    "    print(\"Source: s3://souzy-scratch/lidc-idri/dicom/\")\n",
    "    \n",
    "    try:\n",
    "        import boto3\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        s3 = boto3.client('s3')\n",
    "        bucket = 'souzy-scratch'\n",
    "        prefix = 'lidc-idri/dicom/'\n",
    "        \n",
    "        # List patient directories (limit to 10 patients)\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        patient_dirs = set()\n",
    "        \n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter='/'):\n",
    "            for p in page.get('CommonPrefixes', []):\n",
    "                patient_dirs.add(p['Prefix'])\n",
    "                if len(patient_dirs) >= 10:\n",
    "                    break\n",
    "            if len(patient_dirs) >= 10:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(patient_dirs)} patient directories\")\n",
    "        \n",
    "        # Download files\n",
    "        for patient_prefix in tqdm(sorted(patient_dirs), desc=\"Downloading patients\"):\n",
    "            patient_id = patient_prefix.split('/')[-2]\n",
    "            local_dir = DICOM_DIR / patient_id\n",
    "            local_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for page in paginator.paginate(Bucket=bucket, Prefix=patient_prefix):\n",
    "                for obj in page.get('Contents', []):\n",
    "                    key = obj['Key']\n",
    "                    if key.endswith('.dcm'):\n",
    "                        filename = key.split('/')[-1]\n",
    "                        local_path = local_dir / filename\n",
    "                        if not local_path.exists():\n",
    "                            s3.download_file(bucket, key, str(local_path))\n",
    "        \n",
    "        print(f\"\\nDownload complete\")\n",
    "        existing_dcm = list(DICOM_DIR.rglob(\"*.dcm\"))\n",
    "        print(f\"Total DICOM files: {len(existing_dcm)}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"boto3 not installed. Install with: uv sync --group dev\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        print(\"\\nPlease ensure AWS credentials are configured.\")\n",
    "else:\n",
    "    print(f\"Using existing {len(existing_dcm)} DICOM files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-find-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "\n",
    "def find_dicom_series(root_dir: Path) -> list[tuple[Path, str]]:\n",
    "    \"\"\"Find DICOM series directories and extract patient IDs.\"\"\"\n",
    "    series_list = []\n",
    "    \n",
    "    for series_dir in sorted(root_dir.iterdir()):\n",
    "        if not series_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        dcm_files = list(series_dir.glob(\"*.dcm\"))\n",
    "        if len(dcm_files) < 10:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            ds = pydicom.dcmread(dcm_files[0], stop_before_pixels=True)\n",
    "            patient_id = str(getattr(ds, 'PatientID', series_dir.name[:20]))\n",
    "            patient_id = patient_id.replace(' ', '_').replace('/', '_')\n",
    "        except Exception:\n",
    "            patient_id = series_dir.name[:20].replace('/', '_')\n",
    "        \n",
    "        series_list.append((series_dir, patient_id))\n",
    "    \n",
    "    return series_list\n",
    "\n",
    "dicom_series = find_dicom_series(DICOM_DIR)\n",
    "print(f\"Found {len(dicom_series)} DICOM series\")\n",
    "\n",
    "if dicom_series:\n",
    "    print(\"\\nSample series:\")\n",
    "    for path, pid in dicom_series[:3]:\n",
    "        dcm_count = len(list(path.glob('*.dcm')))\n",
    "        print(f\"  {pid}: {dcm_count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-convert-nifti",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DICOM to NIfTI for MONAI/TorchIO\n",
    "nifti_paths = []\n",
    "\n",
    "if HAVE_SIMPLEITK and dicom_series:\n",
    "    print(\"Converting DICOM to NIfTI...\")\n",
    "    \n",
    "    for series_dir, patient_id in dicom_series:\n",
    "        nifti_path = NIFTI_DIR / f\"{patient_id}.nii.gz\"\n",
    "        nifti_paths.append(nifti_path)\n",
    "        \n",
    "        if nifti_path.exists():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            reader = sitk.ImageSeriesReader()\n",
    "            dicom_names = reader.GetGDCMSeriesFileNames(str(series_dir))\n",
    "            reader.SetFileNames(dicom_names)\n",
    "            image = reader.Execute()\n",
    "            sitk.WriteImage(image, str(nifti_path))\n",
    "            print(f\"  Converted: {patient_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed {patient_id}: {e}\")\n",
    "            nifti_paths.pop()\n",
    "    \n",
    "    print(f\"\\nNIfTI files ready: {len(nifti_paths)}\")\n",
    "else:\n",
    "    # Check for existing NIfTI\n",
    "    nifti_paths = list(NIFTI_DIR.glob(\"*.nii.gz\"))\n",
    "    if nifti_paths:\n",
    "        print(f\"Using existing {len(nifti_paths)} NIfTI files\")\n",
    "    else:\n",
    "        print(\"No NIfTI files available (install SimpleITK for conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-ingest-radiobject",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledb\n",
    "\n",
    "def radiobject_exists(uri: str) -> bool:\n",
    "    try:\n",
    "        if uri.startswith(\"s3://\"):\n",
    "            return tiledb.object_type(uri) == \"group\"\n",
    "        return Path(uri).exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Local RadiObject\n",
    "if radiobject_exists(RADIOBJECT_LOCAL_URI):\n",
    "    print(f\"Local RadiObject exists: {RADIOBJECT_LOCAL_URI}\")\n",
    "    radi_local = RadiObject(RADIOBJECT_LOCAL_URI)\n",
    "elif dicom_series:\n",
    "    print(\"Creating local RadiObject from DICOM...\")\n",
    "    start = time.perf_counter()\n",
    "    radi_local = RadiObject.from_dicoms(\n",
    "        uri=RADIOBJECT_LOCAL_URI,\n",
    "        dicom_dirs=dicom_series,\n",
    "        reorient=True,\n",
    "    )\n",
    "    print(f\"Created in {time.perf_counter() - start:.1f}s\")\n",
    "else:\n",
    "    radi_local = None\n",
    "    print(\"No DICOM data available\")\n",
    "\n",
    "if radi_local:\n",
    "    print(f\"Subjects: {len(radi_local)}\")\n",
    "    print(f\"Collections: {radi_local.collection_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-ingest-s3",
   "metadata": {},
   "outputs": [],
   "source": "# S3 RadiObject - load existing from s3://souzy-scratch/lidc-idri/radiobject\nradi_s3 = None\n\nif RADIOBJECT_S3_URI:\n    configure(\n        s3=S3Config(\n            region=os.environ.get(\"AWS_REGION\", \"us-east-1\"),\n            max_parallel_ops=8,\n        )\n    )\n    \n    if radiobject_exists(RADIOBJECT_S3_URI):\n        print(f\"Loading existing S3 RadiObject: {RADIOBJECT_S3_URI}\")\n        radi_s3 = RadiObject(RADIOBJECT_S3_URI)\n        print(f\"S3 subjects: {len(radi_s3)}\")\n        print(f\"S3 collections: {radi_s3.collection_names}\")\n    else:\n        print(f\"S3 RadiObject not found at: {RADIOBJECT_S3_URI}\")\n        print(\"S3 benchmark will be skipped.\")\nelse:\n    print(\"S3 benchmark disabled (RADIOBJECT_S3_URI not configured)\")"
  },
  {
   "cell_type": "markdown",
   "id": "adapters-header",
   "metadata": {},
   "source": [
    "## 4. Framework Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapters-radiobject",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radiobject_loader(radi: RadiObject) -> DataLoader:\n",
    "    \"\"\"Create RadiObject DataLoader.\"\"\"\n",
    "    return create_training_dataloader(\n",
    "        radi,\n",
    "        modalities=[radi.collection_names[0]],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "if radi_local:\n",
    "    loader = create_radiobject_loader(radi_local)\n",
    "    batch = next(iter(loader))\n",
    "    print(f\"RadiObject batch shape: {batch['image'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapters-monai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monai_loader(nifti_paths: list[Path]) -> DataLoader | None:\n",
    "    \"\"\"Create MONAI DataLoader.\"\"\"\n",
    "    if not HAVE_MONAI or not nifti_paths:\n",
    "        return None\n",
    "    \n",
    "    data_dicts = [{\"image\": str(p)} for p in nifti_paths]\n",
    "    \n",
    "    transforms = Compose([\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        RandSpatialCropd(keys=[\"image\"], roi_size=PATCH_SIZE, random_size=False),\n",
    "    ])\n",
    "    \n",
    "    dataset = MonaiDataset(data=data_dicts, transform=transforms)\n",
    "    return MonaiDataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "if HAVE_MONAI and nifti_paths:\n",
    "    monai_loader = create_monai_loader(nifti_paths)\n",
    "    if monai_loader:\n",
    "        batch = next(iter(monai_loader))\n",
    "        print(f\"MONAI batch shape: {batch['image'].shape}\")\n",
    "else:\n",
    "    print(\"MONAI loader not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapters-torchio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_torchio_loader(nifti_paths: list[Path]) -> DataLoader | None:\n",
    "    \"\"\"Create TorchIO DataLoader.\"\"\"\n",
    "    if not HAVE_TORCHIO or not nifti_paths:\n",
    "        return None\n",
    "    \n",
    "    subjects = [tio.Subject(image=tio.ScalarImage(str(p))) for p in nifti_paths]\n",
    "    \n",
    "    transform = tio.Compose([\n",
    "        tio.CropOrPad(PATCH_SIZE),\n",
    "    ])\n",
    "    \n",
    "    dataset = tio.SubjectsDataset(subjects, transform=transform)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "if HAVE_TORCHIO and nifti_paths:\n",
    "    tio_loader = create_torchio_loader(nifti_paths)\n",
    "    if tio_loader:\n",
    "        batch = next(iter(tio_loader))\n",
    "        print(f\"TorchIO batch shape: {batch['image'][tio.DATA].shape}\")\n",
    "else:\n",
    "    print(\"TorchIO loader not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-a-header",
   "metadata": {},
   "source": [
    "## 5. Benchmark Suite A: Local Storage Comparison\n",
    "\n",
    "Fair comparison - all frameworks reading from local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-a-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_results: dict[str, list[BenchmarkResult]] = {\n",
    "    \"RadiObject\": [],\n",
    "    \"MONAI\": [],\n",
    "    \"TorchIO\": [],\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK SUITE A: LOCAL STORAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    print(f\"\\nRun {run + 1}/{N_RUNS}\")\n",
    "    \n",
    "    # RadiObject\n",
    "    if radi_local:\n",
    "        loader = create_radiobject_loader(radi_local)\n",
    "        result = run_dataloader_benchmark(loader, \"RadiObject\", \"local\")\n",
    "        local_results[\"RadiObject\"].append(result)\n",
    "        print(f\"  RadiObject: {result.throughput:.1f} samples/sec\")\n",
    "        del loader\n",
    "        gc.collect()\n",
    "    \n",
    "    # MONAI\n",
    "    if HAVE_MONAI and nifti_paths:\n",
    "        loader = create_monai_loader(nifti_paths)\n",
    "        if loader:\n",
    "            result = run_dataloader_benchmark(loader, \"MONAI\", \"local\")\n",
    "            local_results[\"MONAI\"].append(result)\n",
    "            print(f\"  MONAI: {result.throughput:.1f} samples/sec\")\n",
    "            del loader\n",
    "            gc.collect()\n",
    "    \n",
    "    # TorchIO\n",
    "    if HAVE_TORCHIO and nifti_paths:\n",
    "        loader = create_torchio_loader(nifti_paths)\n",
    "        if loader:\n",
    "            result = run_dataloader_benchmark(\n",
    "                loader, \"TorchIO\", \"local\", \n",
    "                image_key=\"image\"\n",
    "            )\n",
    "            # TorchIO returns nested structure\n",
    "            local_results[\"TorchIO\"].append(result)\n",
    "            print(f\"  TorchIO: {result.throughput:.1f} samples/sec\")\n",
    "            del loader\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\nBenchmark A complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-b-header",
   "metadata": {},
   "source": [
    "## 6. Benchmark Suite B: S3 vs Local (RadiObject Only)\n",
    "\n",
    "Demonstrates RadiObject's unique S3 capability via TileDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-b-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_results: dict[str, list[BenchmarkResult]] = {\n",
    "    \"Local\": [],\n",
    "    \"S3\": [],\n",
    "}\n",
    "\n",
    "if radi_s3:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BENCHMARK SUITE B: S3 vs LOCAL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for run in range(N_RUNS):\n",
    "        print(f\"\\nRun {run + 1}/{N_RUNS}\")\n",
    "        \n",
    "        # Local\n",
    "        if radi_local:\n",
    "            loader = create_radiobject_loader(radi_local)\n",
    "            result = run_dataloader_benchmark(loader, \"RadiObject\", \"local\")\n",
    "            s3_results[\"Local\"].append(result)\n",
    "            print(f\"  Local: {result.throughput:.1f} samples/sec\")\n",
    "            del loader\n",
    "            gc.collect()\n",
    "        \n",
    "        # S3\n",
    "        loader = create_radiobject_loader(radi_s3)\n",
    "        result = run_dataloader_benchmark(loader, \"RadiObject\", \"s3\")\n",
    "        s3_results[\"S3\"].append(result)\n",
    "        print(f\"  S3: {result.throughput:.1f} samples/sec\")\n",
    "        del loader\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\nBenchmark B complete\")\n",
    "else:\n",
    "    print(\"Skipping S3 benchmark (not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 7. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate local results\n",
    "local_aggregated = {}\n",
    "for framework, results in local_results.items():\n",
    "    if results:\n",
    "        local_aggregated[framework] = aggregate_results(results)\n",
    "\n",
    "# Aggregate S3 results\n",
    "s3_aggregated = {}\n",
    "for scenario, results in s3_results.items():\n",
    "    if results:\n",
    "        s3_aggregated[scenario] = aggregate_results(results)\n",
    "\n",
    "# Display local results\n",
    "print(\"=\" * 60)\n",
    "print(\"LOCAL STORAGE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "local_df_data = []\n",
    "for name, agg in local_aggregated.items():\n",
    "    local_df_data.append({\n",
    "        \"Framework\": name,\n",
    "        \"Throughput (samples/sec)\": f\"{agg.throughput_mean:.1f} +/- {agg.throughput_std:.1f}\",\n",
    "        \"Cold Start (s)\": f\"{agg.cold_start_mean:.3f} +/- {agg.cold_start_std:.3f}\",\n",
    "        \"Batch Time (ms)\": f\"{agg.batch_time_mean*1000:.1f} +/- {agg.batch_time_std*1000:.1f}\",\n",
    "        \"Peak Memory (MB)\": f\"{agg.peak_memory_mean:.1f} +/- {agg.peak_memory_std:.1f}\",\n",
    "    })\n",
    "\n",
    "if local_df_data:\n",
    "    local_df = pd.DataFrame(local_df_data)\n",
    "    display(local_df)\n",
    "\n",
    "# Display S3 results\n",
    "if s3_aggregated:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"S3 vs LOCAL RESULTS (RadiObject)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    s3_df_data = []\n",
    "    for scenario, agg in s3_aggregated.items():\n",
    "        s3_df_data.append({\n",
    "            \"Storage\": scenario,\n",
    "            \"Throughput (samples/sec)\": f\"{agg.throughput_mean:.1f} +/- {agg.throughput_std:.1f}\",\n",
    "            \"Cold Start (s)\": f\"{agg.cold_start_mean:.3f} +/- {agg.cold_start_std:.3f}\",\n",
    "            \"Batch Time (ms)\": f\"{agg.batch_time_mean*1000:.1f} +/- {agg.batch_time_std*1000:.1f}\",\n",
    "        })\n",
    "    \n",
    "    s3_df = pd.DataFrame(s3_df_data)\n",
    "    display(s3_df)\n",
    "    \n",
    "    # Calculate overhead\n",
    "    if \"Local\" in s3_aggregated and \"S3\" in s3_aggregated:\n",
    "        overhead = (s3_aggregated[\"Local\"].throughput_mean / s3_aggregated[\"S3\"].throughput_mean - 1) * 100\n",
    "        print(f\"\\nS3 throughput overhead: {overhead:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 8. Visualization Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorblind-friendly palette\n",
    "COLORS = {\n",
    "    \"RadiObject\": \"#0077BB\",  # Blue\n",
    "    \"MONAI\": \"#EE7733\",       # Orange\n",
    "    \"TorchIO\": \"#009988\",     # Teal\n",
    "    \"Local\": \"#0077BB\",\n",
    "    \"S3\": \"#CC3311\",          # Red\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-throughput",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Local Throughput Comparison\n",
    "if local_aggregated:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    frameworks = list(local_aggregated.keys())\n",
    "    throughputs = [local_aggregated[f].throughput_mean for f in frameworks]\n",
    "    errors = [local_aggregated[f].throughput_std for f in frameworks]\n",
    "    colors = [COLORS.get(f, \"#999999\") for f in frameworks]\n",
    "    \n",
    "    bars = ax.bar(frameworks, throughputs, yerr=errors, capsize=5, color=colors, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_ylabel(\"Throughput (samples/sec)\")\n",
    "    ax.set_title(\"Data Loading Throughput Comparison\")\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, throughputs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f\"{val:.1f}\", ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / \"local_throughput.png\")\n",
    "    print(f\"Saved: {ASSETS_DIR / 'local_throughput.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-s3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. S3 vs Local Comparison\n",
    "if s3_aggregated and len(s3_aggregated) == 2:\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    \n",
    "    scenarios = [\"Local\", \"S3\"]\n",
    "    throughputs = [s3_aggregated[s].throughput_mean for s in scenarios]\n",
    "    errors = [s3_aggregated[s].throughput_std for s in scenarios]\n",
    "    colors = [COLORS[s] for s in scenarios]\n",
    "    \n",
    "    bars = ax.bar(scenarios, throughputs, yerr=errors, capsize=5, color=colors, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_ylabel(\"Throughput (samples/sec)\")\n",
    "    ax.set_title(\"RadiObject: Local vs S3 Storage\")\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, throughputs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f\"{val:.1f}\", ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / \"s3_comparison.png\")\n",
    "    print(f\"Saved: {ASSETS_DIR / 's3_comparison.png'}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"S3 comparison chart skipped (not enough data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Memory Usage Comparison\n",
    "if local_aggregated:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    frameworks = list(local_aggregated.keys())\n",
    "    memories = [local_aggregated[f].peak_memory_mean for f in frameworks]\n",
    "    errors = [local_aggregated[f].peak_memory_std for f in frameworks]\n",
    "    colors = [COLORS.get(f, \"#999999\") for f in frameworks]\n",
    "    \n",
    "    bars = ax.bar(frameworks, memories, yerr=errors, capsize=5, color=colors, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_ylabel(\"Peak Memory (MB)\")\n",
    "    ax.set_title(\"Peak Memory Usage During Data Loading\")\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, memories):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f\"{val:.0f}\", ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / \"memory.png\")\n",
    "    print(f\"Saved: {ASSETS_DIR / 'memory.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-coldstart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cold Start Comparison\n",
    "if local_aggregated:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    frameworks = list(local_aggregated.keys())\n",
    "    cold_starts = [local_aggregated[f].cold_start_mean for f in frameworks]\n",
    "    errors = [local_aggregated[f].cold_start_std for f in frameworks]\n",
    "    colors = [COLORS.get(f, \"#999999\") for f in frameworks]\n",
    "    \n",
    "    bars = ax.bar(frameworks, cold_starts, yerr=errors, capsize=5, color=colors, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_ylabel(\"Cold Start Time (seconds)\")\n",
    "    ax.set_title(\"First Batch Load Time (Cold Start)\")\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, cold_starts):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f\"{val:.2f}s\", ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ASSETS_DIR / \"cold_start.png\")\n",
    "    print(f\"Saved: {ASSETS_DIR / 'cold_start.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 9. Export for README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown tables for README\n",
    "def format_winner(values: dict[str, float], metric: str, higher_is_better: bool = True) -> str:\n",
    "    \"\"\"Determine the winner for a metric.\"\"\"\n",
    "    if not values:\n",
    "        return \"-\"\n",
    "    if higher_is_better:\n",
    "        return max(values, key=values.get)\n",
    "    return min(values, key=values.get)\n",
    "\n",
    "print(\"## Local Storage Benchmark\")\n",
    "print(\"\")\n",
    "print(\"| Metric | \" + \" | \".join(local_aggregated.keys()) + \" | Winner |\")\n",
    "print(\"|--------|\" + \"|\".join([\"--------\"] * (len(local_aggregated) + 1)) + \"|\")\n",
    "\n",
    "if local_aggregated:\n",
    "    # Throughput\n",
    "    throughputs = {f: agg.throughput_mean for f, agg in local_aggregated.items()}\n",
    "    row = \"| Throughput (samples/sec) | \"\n",
    "    row += \" | \".join([f\"{agg.throughput_mean:.1f} +/- {agg.throughput_std:.1f}\" for agg in local_aggregated.values()])\n",
    "    row += f\" | **{format_winner(throughputs, 'throughput', True)}** |\"\n",
    "    print(row)\n",
    "    \n",
    "    # Cold Start\n",
    "    cold_starts = {f: agg.cold_start_mean for f, agg in local_aggregated.items()}\n",
    "    row = \"| Cold Start (s) | \"\n",
    "    row += \" | \".join([f\"{agg.cold_start_mean:.3f} +/- {agg.cold_start_std:.3f}\" for agg in local_aggregated.values()])\n",
    "    row += f\" | **{format_winner(cold_starts, 'cold_start', False)}** |\"\n",
    "    print(row)\n",
    "    \n",
    "    # Memory\n",
    "    memories = {f: agg.peak_memory_mean for f, agg in local_aggregated.items()}\n",
    "    row = \"| Peak Memory (MB) | \"\n",
    "    row += \" | \".join([f\"{agg.peak_memory_mean:.1f} +/- {agg.peak_memory_std:.1f}\" for agg in local_aggregated.values()])\n",
    "    row += f\" | **{format_winner(memories, 'memory', False)}** |\"\n",
    "    print(row)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "if s3_aggregated and len(s3_aggregated) == 2:\n",
    "    print(\"## S3 Performance (RadiObject)\")\n",
    "    print(\"\")\n",
    "    print(\"| Metric | Local | S3 | Overhead |\")\n",
    "    print(\"|--------|-------|-----|----------|\")\n",
    "    \n",
    "    local_tp = s3_aggregated[\"Local\"].throughput_mean\n",
    "    s3_tp = s3_aggregated[\"S3\"].throughput_mean\n",
    "    overhead = (local_tp / s3_tp - 1) * 100\n",
    "    \n",
    "    print(f\"| Throughput (samples/sec) | {local_tp:.1f} | {s3_tp:.1f} | {overhead:.1f}% |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as JSON\n",
    "results_json = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patch_size\": PATCH_SIZE,\n",
    "        \"num_workers\": NUM_WORKERS,\n",
    "        \"n_warmup\": N_WARMUP,\n",
    "        \"n_batches\": N_BATCHES,\n",
    "        \"n_runs\": N_RUNS,\n",
    "    },\n",
    "    \"local_results\": {\n",
    "        f: {\n",
    "            \"throughput_mean\": agg.throughput_mean,\n",
    "            \"throughput_std\": agg.throughput_std,\n",
    "            \"cold_start_mean\": agg.cold_start_mean,\n",
    "            \"cold_start_std\": agg.cold_start_std,\n",
    "            \"batch_time_mean\": agg.batch_time_mean,\n",
    "            \"batch_time_std\": agg.batch_time_std,\n",
    "            \"peak_memory_mean\": agg.peak_memory_mean,\n",
    "            \"peak_memory_std\": agg.peak_memory_std,\n",
    "        }\n",
    "        for f, agg in local_aggregated.items()\n",
    "    },\n",
    "    \"s3_results\": {\n",
    "        s: {\n",
    "            \"throughput_mean\": agg.throughput_mean,\n",
    "            \"throughput_std\": agg.throughput_std,\n",
    "            \"cold_start_mean\": agg.cold_start_mean,\n",
    "            \"cold_start_std\": agg.cold_start_std,\n",
    "        }\n",
    "        for s, agg in s3_aggregated.items()\n",
    "    } if s3_aggregated else {},\n",
    "}\n",
    "\n",
    "results_path = ASSETS_DIR / \"benchmark_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {len(dicom_series) if dicom_series else 'N/A'} DICOM series\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Patch size: {PATCH_SIZE}\")\n",
    "print(f\"Runs per framework: {N_RUNS}\")\n",
    "\n",
    "print(\"\\nFramework Availability:\")\n",
    "print(f\"  RadiObject: Available\")\n",
    "print(f\"  MONAI: {'Available' if HAVE_MONAI else 'Not installed'}\")\n",
    "print(f\"  TorchIO: {'Available' if HAVE_TORCHIO else 'Not installed'}\")\n",
    "\n",
    "print(\"\\nGenerated Artifacts:\")\n",
    "for png in ASSETS_DIR.glob(\"*.png\"):\n",
    "    print(f\"  {png}\")\n",
    "for json_file in ASSETS_DIR.glob(\"*.json\"):\n",
    "    print(f\"  {json_file}\")\n",
    "\n",
    "print(\"\\nRadiObject Advantages:\")\n",
    "print(\"  1. Native DICOM ingestion (no conversion step)\")\n",
    "print(\"  2. Full DICOM metadata preservation\")\n",
    "print(\"  3. S3 native support via TileDB VFS\")\n",
    "print(\"  4. TileDB tile-level caching for repeated access\")\n",
    "print(\"  5. Random sub-volume reads without loading full volume\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## 11. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up generated data\n",
    "# import shutil\n",
    "# \n",
    "# if NIFTI_DIR.exists():\n",
    "#     shutil.rmtree(NIFTI_DIR)\n",
    "#     print(f\"Removed: {NIFTI_DIR}\")\n",
    "# \n",
    "# if Path(RADIOBJECT_LOCAL_URI).exists():\n",
    "#     shutil.rmtree(RADIOBJECT_LOCAL_URI)\n",
    "#     print(f\"Removed: {RADIOBJECT_LOCAL_URI}\")\n",
    "\n",
    "print(\"Cleanup skipped (uncomment to remove generated data)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}